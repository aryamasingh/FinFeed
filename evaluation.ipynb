{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FinFeedRAG\n",
    "import random\n",
    "#Generate evaluation prompts with gpt-3.5-turbo and store them in evaluation_prompts.txt\n",
    "def generate_evaluation_prompts():\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    answer = llm.invoke(\"Please generate a list of 100 questions that someone may ask about the current state of the economy.\")\n",
    "\n",
    "    file = open(\"Evaluation_experiments/evaluation_prompts.txt\", \"w\")\n",
    "    file.write(answer.content)\n",
    "\n",
    "#Populate prompts from evaluation_prompts.txt\n",
    "def load_evaluation_user_prompts():\n",
    "\n",
    "    prompts = []\n",
    "\n",
    "    #Open .txt file\n",
    "    file = open(\"Evaluation_experiments/evaluation_prompts.txt\", \"r\")\n",
    "\n",
    "    #Read the file line by line, strip each lines of leading and trailing numerals, periods, new line symbols, and spaces, and append it to prompts.\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        line = line.strip('0123456789. \\n')\n",
    "        prompts.append(line)\n",
    "\n",
    "    return prompts\n",
    "\n",
    "def combined_bots(user_prompt):\n",
    "\n",
    "    #Base answer\n",
    "    base_model = ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0)\n",
    "    base_answer = base_model.invoke(user_prompt).content\n",
    "\n",
    "    #Our answer\n",
    "\n",
    "    #initialize our model\n",
    "    bot = FinFeedRAG(pine_cone_api_key=os.getenv('PINECONE_API_KEY'), openai_api_key=os.getenv('OPENAI_API_KEY'), pinecone_index='latest-news')\n",
    "    our_answer = bot.chain_for_eval(user_prompt)\n",
    "\n",
    "    #Combine answers\n",
    "    integrating_model = ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0)\n",
    "\n",
    "    template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You will be given two answers provided by different chatbots to the following user prompt:\"),\n",
    "            (\"system\", \"User_prompt: \" + user_prompt),\n",
    "            (\"system\", \"The following are the answers provided by the chatbots:\"),\n",
    "            (\"system\", \"Answer 1: \" + base_answer),\n",
    "            (\"system\", \"Answer 2: \" + our_answer),\n",
    "            (\"system\", \"Please combine both answers into a single, coherent, and comprehenive answer.\")])\n",
    "\n",
    "\n",
    "        \n",
    "    chaining = ({} | template | integrating_model)\n",
    "    combined_answer = chaining.invoke([''])\n",
    "\n",
    "    return combined_answer.content\n",
    "#Evaluation function \n",
    "#This function evaluates our integrated bot against chatgpt_3.5-turbo\n",
    "\n",
    "def model_evaluation(user_prompts, evaluator_prompt):\n",
    "    \n",
    "    our_answers = []\n",
    "    benchmark_answers = []\n",
    "    evaluations = []\n",
    "    preferred_answers = []\n",
    "    \n",
    "    for user_prompt in user_prompts:\n",
    "\n",
    "        #Answers\n",
    "        our_answer = combined_bots(user_prompt)\n",
    "\n",
    "        #Benchmark answer\n",
    "        benchmark_model = ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0)\n",
    "        benchmark_answer = benchmark_model.invoke(user_prompt).content\n",
    "\n",
    "        #Evaluation\n",
    "        template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", evaluator_prompt),\n",
    "            (\"system\", \"User_prompt: \" + user_prompt),\n",
    "            (\"system\", \"Answer 1: \" + benchmark_answer),\n",
    "            (\"system\", \"Answer 2: \" + our_answer)])\n",
    "        \n",
    "        chaining = ({} | template | benchmark_model)\n",
    "        evaluation = chaining.invoke([''])\n",
    "\n",
    "        our_answers.append(our_answer)\n",
    "        benchmark_answers.append(benchmark_answer)\n",
    "        evaluations.append(evaluation.content)\n",
    "\n",
    "    #Extract preferred answer as an integer \n",
    "    for evaluation in evaluations:\n",
    "\n",
    "        #Benchmark answer\n",
    "        evaluator_model = ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0)\n",
    "\n",
    "        #Evaluation\n",
    "        template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"Determine what is the user's preferred answer. Please restrict your output to a single integer.\"),\n",
    "            (\"system\", \"User_prompt: \" + evaluation)])\n",
    "        \n",
    "        chaining = ({} | template | evaluator_model)\n",
    "        preferred_answer = chaining.invoke([''])\n",
    "        preferred_answers.append(int(preferred_answer.content))\n",
    "    \n",
    "    return benchmark_answers, our_answers, evaluations, preferred_answers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate model on prompts\n",
    "prompts = load_evaluation_user_prompts()\n",
    "\n",
    "evaluator_prompt = \"You are a helpful finance bot evaluator. You will be given a user prompt, and the corresponding answers of two different bots. The user prompt is likely concerned with very current matters, so the bots may or may not have access to relevant information. Please e valuate which answer provides the most relevant answer. Based on this, which  do you consider the best answer?.\"#. Please provide a reason for your choice.\"\"#. Please provide a reason for your choice.\"\n",
    "\n",
    "#We only evalaute on 50 prompts chosen randomly. \n",
    "benchmark_answers, our_answers, evaluations, preferred_answers = model_evaluation(random.sample(prompts, 50),evaluator_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store results of the evaluation \n",
    "\n",
    "#Set the expriment number for bookeeping\n",
    "experiment_number = 1\n",
    "\n",
    "#Create a dataframe with results and save to a .pkl file\n",
    "data = {'benchmark_answer':benchmark_answers,'our_answer': our_answers, 'llm_evaluation':evaluations,'preferred_model':preferred_answers}\n",
    "df = pd.DataFrame(data=data)\n",
    "\n",
    "#Add the evaluation prompt as an attribute to the dataframe\n",
    "df.attrs['evaluator_prompt'] = evaluator_prompt\n",
    "\n",
    "df.to_pickle('Evaluation_experiments/Experiment_' + str(experiment_number) + '/results_experiment_' + str(experiment_number) + '.pkl')\n",
    "\n",
    "#Create bar plot\n",
    "labels = ['Benchmark','FinFeed']\n",
    "values = [(df['preferred_model']==1).sum(),(df['preferred_model']==2).sum()]\n",
    "\n",
    "fig = plt.figure(figsize = (5, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(labels, values)\n",
    " \n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Number of times chosen by the evaluator\")\n",
    "plt.title('Results of evaluation')\n",
    "\n",
    "plt.savefig('Evaluation_experiments/Experiment_' + str(experiment_number) + '/barplot_experiment_' + str(experiment_number) + '.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
