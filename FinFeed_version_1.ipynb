{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9cf0832-9171-4918-9e53-723b8e883481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from pinecone import Pinecone,  PodSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "class FinFeedRAG:\n",
    "    def __init__(self, pine_cone_api_key, openai_api_key, pinecone_index, embeddings_model= OpenAIEmbeddings(),model='gpt-3.5-turbo'):\n",
    "        self.openai_api_key=openai_api_key\n",
    "        self.api_key_pinecone = pine_cone_api_key\n",
    "        self.pinecone_index = pinecone_index\n",
    "        # Initialize Pinecone connection\n",
    "        self.vector_db = None\n",
    "        self.embeddings=embeddings_model\n",
    "        self.model=model\n",
    "        self.template = \"\"\"\n",
    "                Answer the question based on the context below. If you can't\n",
    "                answer the question, reply \"I don't know\".\n",
    "                \n",
    "                Context: {context}\n",
    "                \n",
    "                Question: {question}\n",
    "                \"\"\"\n",
    "\n",
    "    def initialize_pinecone(self):\n",
    "        if self.vector_db is None:  # Check if it's already initialized\n",
    "            pc = Pinecone(api_key=self.api_key_pinecone)\n",
    "            self.vector_db = pc.Index(self.pinecone_index)  # Connect to the index and store the connection\n",
    "        return self\n",
    "    \n",
    "    def preprocess_youtube_text(self, text_file, chunksize,chunkoverlap):\n",
    "        loader = TextLoader(text_file) #text instance of langchain\n",
    "        text_documents = loader.load() \n",
    "        # Assuming RecursiveCharacterTextSplitter is a class you have access to or have created\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=chunksize, chunk_overlap=chunkoverlap)\n",
    "        processed_text = splitter.split(text_documents)\n",
    "        # Further processing can be done here if necessary\n",
    "        return processed_text\n",
    "\n",
    "    def upload_to_vb(self,text,embeddings,index=None):\n",
    "        if index is None:\n",
    "            index = self.pinecone_index\n",
    "        return PineconeVectorStore.from_documents(self.preprocess_youtube_text(text), self.embeddings, index_name=index)     \n",
    "\n",
    "\n",
    "    def preprocess_user_input(self, text):\n",
    "        # Simple text preprocessing: lowercasing, removing punctuation\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        return text\n",
    "\n",
    "    def generate_query(self, input_text):\n",
    "        # Preprocess the text\n",
    "        processed_text = self.preprocess_user_input(input_text)    \n",
    "        # Extract keywords based on frequency, assuming more frequent terms are more relevant\n",
    "        words = processed_text.split()\n",
    "        word_freq = Counter(words)\n",
    "        common_words = word_freq.most_common(5)  # Get the top 5 words       \n",
    "        # Form a query by joining the most common words\n",
    "        query = ' '.join(word for word, _ in common_words)\n",
    "        return query\n",
    "\n",
    "     \n",
    "    def retrieve_embeddings(self, query, most_similar=2):\n",
    "        assert self.vector_db is not None, \"Initialize Pinecone first\"\n",
    "        query_result = self.vector_db.query(vector=self.embeddings.embed_query(query), top_k=most_similar)\n",
    "        ids = [item['id'] for item in query_result['matches']]\n",
    "        return [self.vector_db.fetch(ids)['vectors'][id]['values'] for id in ids]\n",
    "\n",
    "\n",
    "    def query_langchain(self, query, most_similar=2,index=None):\n",
    "        if index is None:\n",
    "            index = self.pinecone_index\n",
    "        # Use LangChain to process the query and get results\n",
    "        return PineconeVectorStore.from_existing_index(index_name=self.pinecone_index,embedding=self.embeddings).similarity_search(query,k=most_similar)\n",
    "        \n",
    "\n",
    "    def provide_context(self, query,index=None,most_similar=2):\n",
    "        if index is None:\n",
    "            index = self.pinecone_index\n",
    "        # Provide context to LLM\n",
    "        return PineconeVectorStore.from_existing_index(index_name=index,embedding=self.embeddings).as_retriever(search_type='similarity',\n",
    "                search_kwargs={\n",
    "                'k': most_similar}).invoke(query)\n",
    "        \n",
    "    def prompt(self,template=None):\n",
    "        if template is None:\n",
    "            template = self.template\n",
    "        return ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "    def llm(self,model=None):\n",
    "        if model is None:\n",
    "            model = self.model\n",
    "        return ChatOpenAI(openai_api_key=self.openai_api_key, model=model)\n",
    "        \n",
    "    def parser(self):\n",
    "        return StrOutputParser()\n",
    "\n",
    "    def chain(self,query):\n",
    "        #complete_query = self.prompt().format(context=self.provide_context(query),question=query)\n",
    "        #response = self.llm().invoke(complete_query)\n",
    "        #return self.parser().invoke(response)\n",
    "        chaining = (\n",
    "        {\"context\": PineconeVectorStore.from_existing_index(index_name=self.pinecone_index,embedding=self.embeddings).as_retriever(search_type='similarity',\n",
    "                search_kwargs={\n",
    "                'k': 4}), \n",
    "         \"question\": RunnablePassthrough()}\n",
    "        | self.prompt()\n",
    "        | self.llm()\n",
    "        | self.parser())\n",
    "        return chaining.invoke(query)\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
